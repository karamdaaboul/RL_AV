{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG\n",
    "\n",
    "In this Jupyter notebook I will try to show the differences between a normal critic-actor algorithm and DDPG and how we can use the DDPG Function from `stable baselines`. Finally, I will use `Optuna` to optimize the hyper parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Differences in algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Cirtic\n",
    "```python\n",
    "def mlp(x, hidden_sizes=(32,), activation=tf.tanh, output_activation=None):\n",
    "    for h in hidden_sizes[:-1]:\n",
    "        x = tf.layers.dense(x, units=h, activation=activation)\n",
    "    return tf.layers.dense(x, units=hidden_sizes[-1], activation=output_activation)\n",
    "\n",
    "def mlp_gaussian_policy(x, a, hidden_sizes, activation, output_activation, action_space):\n",
    "    act_dim = a.shape.as_list()[-1]\n",
    "    mu = mlp(x, list(hidden_sizes)+[act_dim], activation, output_activation)\n",
    "    log_std = tf.get_variable(name='log_std', initializer=-0.5*np.ones(act_dim, dtype=np.float32))\n",
    "    std = tf.exp(log_std)\n",
    "    pi = mu + tf.random_normal(tf.shape(mu)) * std\n",
    "    logp = gaussian_likelihood(a, mu, log_std)\n",
    "    logp_pi = gaussian_likelihood(pi, mu, log_std)\n",
    "    return pi, logp, logp_pi\n",
    "\n",
    "def mlp_actor_critic(x, a, hidden_sizes=(64,64), activation=tf.tanh, \n",
    "                     output_activation=None, policy=None, action_space=None):\n",
    "    # Actor\n",
    "    policy = mlp_gaussian_policy\n",
    "    with tf.variable_scope('pi'):\n",
    "        pi, logp, logp_pi = policy(x, a, hidden_sizes, activation, output_activation, action_space)\n",
    "    # Critic\n",
    "    with tf.variable_scope('v'):\n",
    "        v = tf.squeeze(mlp(x, list(hidden_sizes)+[1], activation, None), axis=1)\n",
    "    return pi, logp, logp_pi, v\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPG\n",
    "```python\n",
    "def mlp(x, hidden_sizes=(32,), activation=tf.tanh, output_activation=None):\n",
    "    for h in hidden_sizes[:-1]:\n",
    "        x = tf.layers.dense(x, units=h, activation=activation)\n",
    "    return tf.layers.dense(x, units=hidden_sizes[-1], activation=output_activation)\n",
    "\n",
    "def mlp_actor_critic(x, a, hidden_sizes=(400,300), activation=tf.nn.relu, \n",
    "                     output_activation=tf.tanh, action_space=None):\n",
    "    act_dim = a.shape.as_list()[-1]\n",
    "    act_limit = action_space.high[0]\n",
    "    # Actor\n",
    "    with tf.variable_scope('pi'):\n",
    "        pi = act_limit * mlp(x, list(hidden_sizes)+[act_dim], activation, output_activation)\n",
    "    # Critic (inputs used action and state)\n",
    "    with tf.variable_scope('q'):\n",
    "        q = tf.squeeze(mlp(tf.concat([x,a], axis=-1), list(hidden_sizes)+[1], activation, None), axis=1)\n",
    "    # Critic (inputs action from the policy and state)\n",
    "    with tf.variable_scope('q', reuse=True):\n",
    "        q_pi = tf.squeeze(mlp(tf.concat([x,pi], axis=-1), list(hidden_sizes)+[1], activation, None), axis=1)\n",
    "    return pi, q, q_pi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Differences in the Cost Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Critic objectives:\n",
    "```python\n",
    "# VPG objectives:\n",
    "# Use Policy gradinet method to train the policy\n",
    "pi_loss = -tf.reduce_mean(logp * adv_ph)\n",
    "# Train the critic: The critic should \n",
    "# give the summ of reward as output\n",
    "v_loss = tf.reduce_mean((ret_ph - v)**2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPG objectives:\n",
    "```python\n",
    "## DDPG losses\n",
    "# Train the policy\n",
    "pi_loss = -tf.reduce_mean(q_pi)\n",
    "\n",
    "# Train the critic based on the used action in the trajectory \n",
    "# Bellman backup for Q function,  use the policy to calculate the target for the q function\n",
    "backup = tf.stop_gradient(r_ph + gamma*(1-d_ph)*q_pi_targ)\n",
    "q_loss = tf.reduce_mean((q-backup)**2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Example with stable_baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import tensorflow as tf\n",
    "from stable_baselines.ddpg.policies import MlpPolicy\n",
    "from stable_baselines.ddpg.policies import FeedForwardPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.ddpg.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise, AdaptiveParamNoiseSpec\n",
    "from stable_baselines import DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"MountainCarContinuous-v0\"\n",
    "# alg \n",
    "algo = \"ddpg\"\n",
    "# Tensorboard log dir'\n",
    "tensorboard_log = 'logs'\n",
    "# Path to a pretrained agent to continue training\n",
    "trained_agent = 'trained_agents'\n",
    "# log-folder\n",
    "log_folder = 'logs'\n",
    "# Seed\n",
    "seed = 0\n",
    "# Verbose mode (0: no output, 1: INFO)\n",
    "verbose = 1\n",
    "# Override log interval (default: -1, no change)\n",
    "log_interval = 6\n",
    "# Evaluation \n",
    "evaluation = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed\n",
    "set the seed for python random, tensorflow, numpy and gym spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seeds(seed):\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_global_seeds(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparams\n",
    "\n",
    "Some parameters that we maybe need to define:\n",
    "* **policy** – (DDPGPolicy or str) The policy model to use (MlpPolicy, CnnPolicy, LnMlpPolicy, …)\n",
    "* **env** – (Gym environment or str) The environment to learn from (if registered in Gym, can be str)\n",
    "* **gamma** – (float) the discount factor\n",
    "* **eval_env** – (Gym Environment) the evaluation environment (can be None)\n",
    "* **nb_train_steps** – (int) the number of training steps (how many times we sample batches from Replay Buffer)\n",
    "* **nb_rollout_steps** – (int) the number of rollout(epsiode) steps\n",
    "* **nb_eval_steps** – (int) the number of evalutation steps\n",
    "* **param_noise** – (AdaptiveParamNoiseSpec) the parameter noise type (can be None)\n",
    "* **action_noise** – (ActionNoise) the action noise type (can be None)\n",
    "* **param_noise_adaption_interval** – (int) apply param noise every N steps\n",
    "* **tau** – (float) the soft update coefficient (keep old values, between 0 and 1)\n",
    "* **normalize_returns** – (bool) should the critic output be normalized\n",
    "* **normalize_observations** – (bool) should the observation be normalized\n",
    "* **batch_size** – (int) the size of the batch for learning the policy\n",
    "* **observation_range** – (tuple) the bounding values for the observation\n",
    "* **return_range** – (tuple) the bounding values for the critic output\n",
    "* **critic_l2_reg** – (float) l2 regularizer coefficient\n",
    "* **actor_lr** – (float) the actor learning rate\n",
    "* **critic_lr** – (float) the critic learning rate\n",
    "* **clip_norm** – (float) clip the gradients (disabled if None)\n",
    "* **reward_scale** – (float) the value the reward should be scaled by\n",
    "* **render** – (bool) enable rendering of the environment\n",
    "* **render_eval** – (bool) enable rendering of the evalution environment\n",
    "* **buffer_size** – (int) the max number of transitions to store, size of the replay buffer\n",
    "* **random_exploration** – (float) Probability of taking a random action (as in an epsilon-greedy strategy) This is not needed for DDPG normally but can help exploring when using HER + DDPG. This hack was present in the original OpenAI Baselines repo (DDPG + HER)\n",
    "* **verbose** – (int) the verbosity level: 0 none, 1 training information, 2 tensorflow debug\n",
    "* **tensorboard_log** – (str) the log location for tensorboard (if None, no logging)\n",
    "* **policy_kwargs** – (dict) additional arguments to be passed to the policy on creation\n",
    "* **full_tensorboard_log** – (bool) enable additional logging when using tensorboard WARNING: this logging can take a lot of space quickly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hyperparameters from yaml file\n",
    "with open('hyperparams/{}.yml'.format(algo), 'r') as f:\n",
    "    hyperparams_dict = yaml.load(f)\n",
    "    hyperparams = hyperparams_dict[env_id]\n",
    "\n",
    "\n",
    "# Sort hyperparams that will be saved\n",
    "saved_hyperparams = OrderedDict([(key, hyperparams[key]) for key in sorted(hyperparams.keys())])\n",
    "print(saved_hyperparams)\n",
    "\n",
    "total_timesteps = hyperparams['total_timesteps']\n",
    "del hyperparams['total_timesteps']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "Reward is 100 for reaching the target of the hill on the right hand side, minus the squared sum of actions from start to goal.\n",
    "\n",
    "This reward function raises an exploration challenge, because if the agent does not reach the target soon enough, it will figure out that it is better not to move, and won't find the target anymore.\n",
    "\n",
    "Note that this reward is unusual with respect to most published work, where the goal was to reach the target as fast as possible, hence favouring a bang-bang strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env():\n",
    "    \"\"\"\n",
    "    Create the environment and wrap it if necessary\n",
    "    :return: (gym.Env)\n",
    "    \"\"\"\n",
    "    global hyperparams\n",
    "\n",
    "    env = gym.make(env_id)\n",
    "    env.seed(seed)\n",
    "    #if env_wrapper is not None:\n",
    "    #    env = env_wrapper(env)\n",
    "    \n",
    "    env = DummyVecEnv([lambda:env])\n",
    "    # Optional Frame-stacking\n",
    "    if hyperparams.get('frame_stack', False):\n",
    "        n_stack = hyperparams['frame_stack']\n",
    "        env = VecFrameStack(env, n_stack)\n",
    "        print(\"Stacking {} frames\".format(n_stack))\n",
    "        del hyperparams['frame_stack']\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "DDPG trains a deterministic policy in an off-policy way. Because the policy is deterministic, if the agent were to explore on-policy, in the beginning it would probably not try a wide enough variety of actions to find useful learning signals. To make DDPG policies explore better, we add noise to their actions at training time. The authors of the original DDPG paper recommended time-correlated OU noise. To facilitate getting higher-quality training data, you may reduce the scale of the noise over the course of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyperparams.get('noise_type')  is not None:\n",
    "    noise_type = hyperparams['noise_type'].strip()\n",
    "    noise_std = hyperparams['noise_std']\n",
    "    n_actions = env.action_space.shape[0]\n",
    "    \n",
    "if 'adaptive-param' in noise_type:\n",
    "    hyperparams['param_noise'] = AdaptiveParamNoiseSpec(initial_stddev=noise_std,\n",
    "                                                        desired_action_stddev=noise_std)\n",
    "elif 'normal' in noise_type:\n",
    "    if 'lin' in noise_type:\n",
    "        hyperparams['action_noise'] = LinearNormalActionNoise(mean=np.zeros(n_actions),\n",
    "                                                              sigma=noise_std * np.ones(n_actions),\n",
    "                                                              final_sigma=hyperparams.get('noise_std_final', 0.0) * np.ones(n_actions),\n",
    "                                                              max_steps=total_timesteps)\n",
    "    else:\n",
    "        hyperparams['action_noise'] = NormalActionNoise(mean=np.zeros(n_actions),\n",
    "                                                                    sigma=noise_std * np.ones(n_actions))\n",
    "elif 'ornstein-uhlenbeck' in noise_type:\n",
    "    hyperparams['action_noise'] = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions),\n",
    "                                                               sigma=noise_std * np.ones(n_actions))\n",
    "    \n",
    "del hyperparams['noise_type']\n",
    "del hyperparams['noise_std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation == True:\n",
    "    # Environment for evaluation\n",
    "    eval_env = gym.make('MountainCarContinuous-v0')\n",
    "    hyperparams['nb_eval_steps'] = 2\n",
    "    hyperparams['render_eval']   = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Custom Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom MLP policy of two layers of size 16 each\n",
    "class CustomPolicy(FeedForwardPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomPolicy, self).__init__(*args, **kwargs,\n",
    "                                           layers=[32, 32,32],\n",
    "                                           layer_norm=False,\n",
    "                                           feature_extraction=\"mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a  Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an agent from scratch\n",
    "tensorboard_log = os.path.join(tensorboard_log, env_id)\n",
    "\n",
    "if evaluation == True:\n",
    "    # Environment for evaluation\n",
    "    model = DDPG(env=env,eval_env = eval_env, tensorboard_log=tensorboard_log, verbose=verbose, **hyperparams)\n",
    "else:\n",
    "    model = DDPG(env=env, tensorboard_log=tensorboard_log, verbose=verbose, **hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model\n",
    "* **total_timesteps** – (int) The total number of samples to train on\n",
    "* **callback** – (function (dict, dict)) -> boolean function called at every steps with state of the algorithm. It takes the local and global variables. If it returns False, training is aborted.\n",
    "* **seed** – (int) The initial seed for training, if None: keep current seed\n",
    "* **log_interval** – (int) The number of timesteps before logging.\n",
    "* **tb_log_name** – (str) the name of the run for tensorboard log\n",
    "reset_num_timesteps – (bool) whether or not to reset the current timestep number (used in logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {}\n",
    "if log_interval > -1:\n",
    "    kwargs = {'log_interval': log_interval}\n",
    "\n",
    "model.learn(total_timesteps=total_timesteps, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = \"{}/{}/\".format(log_folder,algo)\n",
    "save_path = os.path.join(log_path, \"{}_{}\".format(env_id,0)) #get_latest_run_id(log_path, env_id) + 1))\n",
    "print(\"Saving to {}\".format(save_path))\n",
    "model.save(\"{}/{}\".format(save_path, env_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test trained Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pretrained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(4):\n",
    "    obs = env.reset()\n",
    "    for i in range(1000):\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "        env.render()\n",
    "        if dones:\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "\n",
    "trained_agent = \"logs/ddpg/MountainCarContinuous-v0_0/MountainCarContinuous-v0.pkl\"\n",
    "\n",
    "if trained_agent.endswith('.pkl') and os.path.isfile(trained_agent):\n",
    "    # Continue training\n",
    "    print(\"Loading pretrained agent\")\n",
    "    # Policy should not be changed\n",
    "    #del hyperparams['policy']\n",
    "\n",
    "    model = DDPG.load(trained_agent, env=env,\n",
    "                       tensorboard_log=tensorboard_log, verbose=verbose, **hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.pruners import SuccessiveHalvingPruner, MedianPruner\n",
    "from optuna.samplers import RandomSampler, TPESampler\n",
    "from optuna.integration.skopt import SkoptSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_py3",
   "language": "python",
   "name": "venv_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
