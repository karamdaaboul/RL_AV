{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL_Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create the virtual environment where we want to save all the needed packages. \n",
    "1. Create a Python 3 virtual environment  \n",
    "`virtualenv ~/no_backup/venv_ml -p python3.5`\n",
    "\n",
    "2. Activate the virtual environment by running the command  \n",
    "`source ~/no_backup/venv_ml/bin/activate`\n",
    "\n",
    "3. Install the required packages   \n",
    "`pip3 install -r ~/no_backup/RL_AV/requirements.txt` \n",
    "\n",
    "4. Show a list of all installed packages  \n",
    "`pip3 list --local`\n",
    "\n",
    "5. install the activated environment as an ipython kernel  \n",
    "`ipython kernel install --user --name=venv_ml`\n",
    "\n",
    "6. Deactivate the environment, When you finish your work   \n",
    "`deactivate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAi Gym\n",
    "\n",
    "CarRacing-v0 is easiest continuous control task to learn from pixels, a top-down racing environment. Discreet control is reasonable in this environment as well, on/off discretisation is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install gym with **CarRacing** environments using the command  \n",
    "`pip install 'gym[box2d-py]'`\n",
    "\n",
    "Now you can import gym and creat your environemt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "# Create and wrap the environment\n",
    "env = gym.make('CarRacing-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gym.openai.com/envs/#box2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action space\n",
    "The action space is the set of triples $(s, a, d) ∈ [−1, 1] × [0, 1] × [0, 1]$, where the steering coefficient $s$ ranges from hard left to hard right, the acceleration $a$ ranges from none to full steam ahead, and the deceleration $d$ ranges from none to slamming the brakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(3,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box(3,) means that the action space of our system has 3 actions that are continuous. If we use the same command for env like CartPole we get Discrete(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of actions which we can control\n",
    "env.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30703196, 0.03470271, 0.57537013], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a sample\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to get the range of the action space we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1.]\n",
      "[-1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.high)\n",
    "print(env.action_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation Space\n",
    "The observation is a 96 × 96 × 3 grid of RGB values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(96, 96, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box(96, 96, 3) is an rgp image. Let's rest the system and see the initail observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Function:\n",
    "Reward is -0.1 every frame and $\\frac{+1000}{N}$ for every track tile visited, where N is the total number of tiles in track. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
=======
    "## Make a Step:\n",
    "Episode finishes when all tiles are visited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 994..1247 -> 253-tiles track\n",
      "The shape of the observiation:  (96, 96, 3)\n",
      "Reward:  7.836507936507937\n",
      "Is done:  False\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "# Sample an action\n",
    "action = env.action_space.sample()\n",
    "# Get the next observation, reward and the done flag from the area. \n",
    "observation, reward, done, _ = env.step(action)\n",
    "print(\"The shape of the observiation: \", np.shape(observation))\n",
    "print(\"Reward: \", reward)\n",
    "print(\"Is done: \", done)\n",
    "# Close the rendering\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
>>>>>>> d440ea5e8c2d83341433225c3d152e67e6134d29
    "## Random politics\n",
    "Let's try to move the car with the simplest policy by moving the car with actions sampled from the action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1188..1489 -> 301-tiles track\n",
      "Episode finished after 1000 timesteps\n",
      "Track generation: 1168..1464 -> 296-tiles track\n",
      "Episode finished after 1000 timesteps\n",
      "Track generation: 1137..1433 -> 296-tiles track\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-89536b2e3f33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# render the environment at each step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;31m# move the car using the sampled actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/venv_ml_prak/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/venv_ml_prak/lib/python3.5/site-packages/gym/envs/box2d/car_racing.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglViewport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVP_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVP_H\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_road\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monetime_geoms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mgeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/venv_ml_prak/lib/python3.5/site-packages/gym/envs/box2d/car_racing.py\u001b[0m in \u001b[0;36mrender_road\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglVertex3f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m                 \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglVertex3f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m                 \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglVertex3f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpoly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroad_poly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglColor4f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/venv_ml_prak/lib/python3.5/site-packages/pyglet/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_module_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i_episode in range(5):\n",
    "    # Get initial observation \n",
    "    observation = env.reset()\n",
    "    for t in range(1000):\n",
    "        # render the environment at each step\n",
    "        env.render()\n",
    "        # move the car using the sampled actions\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "# Close the rendering\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some indicators shown at the bottom of the window and the state RGB buffer. From left to right: true speed, four ABS sensors, steering wheel position, gyroscope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAi Baselines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stable Baselines maybe is better choise than OpenAi Baselines but if you want to install the OpenAi Baselines use the following steps\n",
    "1. git clone https://github.com/openai/baselines.git\n",
    "2. remove 'gym>=0.10.0, <1.0.0' from install_requires in setup.py\n",
    "```python\n",
    "install_requires=[\n",
    "         'gym>=0.10.0, <1.0.0',\n",
    "         'scipy',\n",
    "         'tqdm',\n",
    "         'joblib',\n",
    "         'cloudpickle',\n",
    "         'click',\n",
    "         'opencv-python'\n",
    "     ],\n",
    "```\n",
    "\n",
    "3. `pip install -e .`\n",
    "\n",
    "If all goes well, you should see this.\n",
    "Successfully installed baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training models with OpenAi Baselines \n",
    "The first step which we should do is to export the logger path. For all algorithms in baselines, the summery data is stored in a folder defined by the logger.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./logs’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir ./logs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You can create a directory where you want to save the trained  model (The weights of the network) to use them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!OPENAI_LOGDIR=$~/no_backup/RL_AV/logs/carracing-a2c OPENAI_LOG_FORMAT=csv python -m baselines.run --alg=a2c  --env=CarRacing-v0 --network=cnn  --num_timesteps=1e4"
=======
    "!mkdir ./model"
>>>>>>> d440ea5e8c2d83341433225c3d152e67e6134d29
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the training you can run ` baselines.run` with different arguments:\n",
    "* --alg: The name of the used algorithm to train the agent (a2c ,ppo2, ...)\n",
    "* --env: the name of openAi Gym Environement (CarRacing, Humanoid,....)\n",
    "* --network: The network used to approximate Policy/ Vaule function (MLP, CNN,...)\n",
    "* --num_timesteps: The number of timesteps\n",
    "* --save_path and --load_path: loads the tensorflow state from a given path before training, and saves it after the training,"
   ]
  },
  {
<<<<<<< HEAD
=======
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env_type: box2d\n",
      "2019-05-29 18:01:21.623787: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-05-29 18:01:21.626790: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz\n",
      "2019-05-29 18:01:21.627228: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x4834a50 executing computations on platform Host. Devices:\n",
      "2019-05-29 18:01:21.627254: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "Training a2c on box2d:CarRacing-v0 with arguments \n",
      "{'network': 'cnn', 'nsteps': 32}\n",
      "WARNING:tensorflow:From /home/karam/workspaces/reinforcement_learning/baselines/baselines/common/input.py:57: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/karam/workspaces/venv_ml_prak/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/karam/workspaces/reinforcement_learning/baselines/baselines/common/policies.py:43: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/karam/workspaces/venv_ml_prak/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Track generation: 1375..1723 -> 348-tiles track\n"
     ]
    }
   ],
   "source": [
    "!OPENAI_LOGDIR=./logs/carracing_a2c OPENAI_LOG_FORMAT=csv python -m baselines.run --alg=a2c --env=CarRacing-v0 --network=cnn --num_timesteps=300 --nsteps=32 --save_path=./model/carracing_30000_a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and visualizing the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselines.common import plot_util as pu\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># {\"env_id\": \"CarRacing-v0\"</th>\n",
       "      <th>\"t_start\": 1559145681.63662}</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>l</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  # {\"env_id\": \"CarRacing-v0\"  \"t_start\": 1559145681.63662} \n",
       "r                           l                              t"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./logs/carracing_a2c/0.0.monitor.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eplenmean</th>\n",
       "      <th>eprewmean</th>\n",
       "      <th>explained_variance</th>\n",
       "      <th>fps</th>\n",
       "      <th>nupdates</th>\n",
       "      <th>policy_entropy</th>\n",
       "      <th>total_timesteps</th>\n",
       "      <th>value_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022072</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>4.256815</td>\n",
       "      <td>32</td>\n",
       "      <td>3.18424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eplenmean  eprewmean  explained_variance  fps  nupdates  policy_entropy  \\\n",
       "0        NaN        NaN            0.022072   61         1        4.256815   \n",
       "\n",
       "   total_timesteps  value_loss  \n",
       "0               32     3.18424  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_prog = pd.read_csv('./logs/carracing_a2c/progress.csv')\n",
    "data_prog.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "* **eplenmean:** Episode length mean.\n",
    "* **eprewmean:** Episode reward mean.\n",
    "* **fps:**  Frame per second.\n",
    "* **nupdates:** Timesteps divided by number of batches.\n",
    "* **policy_entropy:**  \n",
    "* **total_timesteps:**  Number of timesteps (i.e. number of actions taken in the environment).\n",
    "* **value_loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bаselines provides helper functions to load the summaries of the results as pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karam/workspaces/reinforcement_learning/baselines/baselines/bench/monitor.py:163: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  df.headers = headers # HACK to preserve backwards compatibility\n"
     ]
    }
   ],
   "source": [
    "results = pu.load_results('./logs/carracing_a2c')\n",
    "r = results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A learning curve from a single run can be plotted as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.cumsum(r.monitor.l), r.monitor.r)\n",
    "plt.title(\"The lengths of the episodes\")\n",
    "plt.xlabel(\"num_timesteps\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.cumsum(r.monitor.l), pu.smooth(r.monitor.r, radius=10))\n",
    "plt.title(\"The lengths of the episodes\")\n",
    "plt.xlabel(\"num_timesteps\");\n"
   ]
  },
  {
>>>>>>> d440ea5e8c2d83341433225c3d152e67e6134d29
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAi Stable Baselines \n",
    "A stable baseline is a good adaptation to OpenAi Baseline. bYou only need one of them.\n",
    "\n",
    "To install Stable Baselines only you should run  \n",
    "`pip install stable-baselines`  \n",
    "to see more use the [link](https://stable-baselines.readthedocs.io/en/master/guide/install.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Method:\n",
    "To have some fun, we will implement the Policy Gradient Method (Actor Critic) from scratch :) :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network:\n",
    "Build the a feedforward neural network to approximate the dynamic model (model-based-RL), the Policy or the Value function (model-free-RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_sizes, scope  ,activation=tf.tanh, output_activation=None):\n",
    "    with tf.variable_scope(scope):\n",
    "        for h in hidden_sizes[:-1]:\n",
    "            x = tf.layers.dense(x, units=h, activation=activation)\n",
    "        out = tf.layers.dense(x, units=hidden_sizes[-1], activation=output_activation)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policies in RL\n",
    "There are two types of policies for reinforcmenet learning: \n",
    "* **Categorical Policy:** Choose an action from a variety of discrete actions.\n",
    "* **gaussian_policy:** Sample of an action from a Gauissan distribution of possible actions (continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_categorical_policy(x, a, hidden_sizes, activation, output_activation, action_space):\n",
    "    # The number of network outputs is equal to the number of Environment Actions.\n",
    "    act_dim = action_space.n\n",
    "    # Approximate the network using FNN \n",
    "    logits = mlp(x, list(hidden_sizes)+[act_dim], activation, None)\n",
    "    # use a multinomial distribution to sample one of the action \n",
    "    # the bigger value of logits is more to be sampled.    \n",
    "    pi = tf.squeeze(tf.multinomial(logits,1), axis=1)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_gaussian_policy(x, a, hidden_sizes, activation, output_activation, action_space):\n",
    "    act_dim = a.shape.as_list()[-1]\n",
    "    mu = mlp(x, list(hidden_sizes)+[act_dim], activation, output_activation)\n",
    "    log_std = tf.get_variable(name='log_std', initializer=-0.5*np.ones(act_dim, dtype=np.float32))\n",
    "    std = tf.exp(log_std)\n",
    "    pi = mu + tf.random_normal(tf.shape(mu)) * std\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critics\n",
    "Policy network (Actor) to select the optimal action and value function network (Critic) to evaluate these actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_actor_critic(x, a, hidden_sizes=(64,64), activation=tf.tanh, \n",
    "                     output_activation=None, policy=None, action_space=None):\n",
    "\n",
    "    # default policy builder depends on action space  (Continuous)\n",
    "    if policy is None and isinstance(action_space, Box):\n",
    "        policy = mlp_gaussian_policy\n",
    "    # Discrete\n",
    "    elif policy is None and isinstance(action_space, Discrete):\n",
    "        policy = mlp_categorical_policy\n",
    "    \n",
    "    # The Actor\n",
    "    with tf.variable_scope('pi'):\n",
    "        pi = policy(x, a, hidden_sizes, activation, output_activation, action_space)\n",
    "        \n",
    "    # The Critic\n",
    "    with tf.variable_scope('v'):\n",
    "        v = tf.squeeze(mlp(x, list(hidden_sizes)+[1], activation, None), axis=1)\n",
    "        \n",
    "    return pi, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trian the Policy:\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Wrapper:\n",
    "To use Carla Simualtion with \"OpenAi Baselines\", we need an interface (wrapper) that defines the Carla Simulator as OpenAi Gym environment. \n",
    "\n",
    "You should not create your interface as it requires a lot of work and there are many good implementations on Github. We will try to find the best for you, but you may need to understand part of it because you need to customize it to achieve the goal of your algorithm. \n",
    "\n",
    "The following thing you may need to customize \n",
    "* Observation room,\n",
    "* Reward function\n",
    "* Flage done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Reward Function\n",
    "The reward used for training in the original [CARLA](http://proceedings.mlr.press/v78/dosovitskiy17a/dosovitskiy17a.pdf) paper is a weighted sum of five terms: \n",
    "* Distance traveled towards the goal d in km,\n",
    "* Speed v in km/h,\n",
    "* Collision damage c,\n",
    "* Intersection with the sidewalk s (between 0 and 1), and\n",
    "* iIntersection with the opposite lane o (between 0 and 1)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(self, measurements, target, direction, action, env_state):\n",
    "    \n",
    "    # Distance towards goal (in km)\n",
    "    d_x = measurements.player_measurements.transform.location.x\n",
    "    d_y = measurements.player_measurements.transform.location.y\n",
    "    d_z = measurements.player_measurements.transform.location.z\n",
    "    player_location = np.array([d_x, d_y, d_z])\n",
    "    goal_location = np.array([target.location.x,\n",
    "                              target.location.y,\n",
    "                              target.location.z])\n",
    "    d = np.linalg.norm(player_location - goal_location) / 1000\n",
    "\n",
    "    # Speed\n",
    "    v = measurements.player_measurements.forward_speed * 3.6\n",
    "    # Collision damage\n",
    "    c_v = measurements.player_measurements.collision_vehicles\n",
    "    c_p = measurements.player_measurements.collision_pedestrians\n",
    "    c_o = measurements.player_measurements.collision_other\n",
    "    c = c_v + c_p + c_o\n",
    "\n",
    "    # Intersection with sidewalk\n",
    "    s = measurements.player_measurements.intersection_offroad\n",
    "\n",
    "    # Intersection with opposite lane\n",
    "    o = measurements.player_measurements.intersection_otherlane\n",
    "\n",
    "    # Compute reward\n",
    "    r = 0\n",
    "    if self.state is not None:\n",
    "        r += 1000 * (self.state['d'] - d)\n",
    "        r += 0.05 * (v - self.state['v'])\n",
    "        r -= 0.00002 * (c - self.state['c'])\n",
    "        r -= -0.1 * float(s > 0.001)\n",
    "        r -= 2 * (o - self.state['o'])\n",
    "\n",
    "    # Update state\n",
    "    new_state = {'d': d, 'v': v, 'c': c, 's': s, 'o': o,\n",
    "                 'd_x': d_x, 'd_y': d_y, 'd_z': d_z,\n",
    "                 'c_v': c_v, 'c_p': c_p, 'c_o': c_o}\n",
    "    self.state = new_state\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REF:\n",
    "* https://github.com/openai/baselines/blob/master/README.md#training-models\n",
    "* https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/README.md\n",
    "* https://github.com/openai/baselines/blob/master/docs/viz/viz.ipynb\n",
    "* https://github.com/openai/baselines/blob/a89bee3c8dc9dde97e32ad7be32d7db31245c3fe/baselines/deepq/experiments/custom_cartpole.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml_prak",
   "language": "python",
   "name": "venv_ml_prak"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
