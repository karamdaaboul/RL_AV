{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Wrappers?\n",
    "\n",
    "Very frequently, you will want to extend the environment’s functionality in some generic way.\n",
    "\n",
    "**Example 1:** An environment gives you some observations, but you want to accumulate them in some buffer and provide to the agent the N last observations, which is a common scenario for dynamic computer games, when one single frame is just not enough to get full information about the game state.\n",
    "\n",
    "**Example 2:** If you want to be able to crop or preprocess an image’s pixels to make it more convenient for the agent to digest, or if you want to normalize reward scores somehow.\n",
    "\n",
    "Therefore you’d like to “wrap” the existing environment and add some extra logic doing something.\n",
    "\n",
    "* **ObservationWrapper:** You need to redefine its observation(obs) method. Argument obs is an observation from the wrapped environment, and this method should return the observation which will be given to the agent.\n",
    "\n",
    "* **RewardWrapper:** Exposes the method reward(rew), which could modify the reward value given to the agent.\n",
    "\n",
    "* **ActionWrapper:** You need to override the method action(act) which could tweak the action passed to the wrapped environment to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym.spaces.box import Box\n",
    "import matplotlib.pyplot as plt\n",
    "env = gym.make(\"CarRacing-v0\")\n",
    "(\"The shape of observation space\",env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a cropped and down sampled image where the background is erased\n",
    "class PreProcessObservation(gym.ObservationWrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        # Define a new Box\n",
    "        self.observation_space = Box(self.observation_space.low[0,0,0],self.observation_space.high[0,0,0],\n",
    "            [40, 48, 1]  # Channel, Width, Height\n",
    "        )\n",
    "\n",
    "    def observation(self, observation):\n",
    "\n",
    "        I = observation[0:80]  # crop\n",
    "        I = 0.2989 * I[:,:,0] + 0.5879 * I[:,:,1] + 0.1140 * I[:,:,2] # Grey Image\n",
    "        I = I[::2, ::2]  # down sample by factor of 2\n",
    "        \n",
    "        return I.astype(np.float32)[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PreProcessObservation(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "total_reward = 0.0\n",
    "\n",
    "for i in range(100):\n",
    "    # Sample an action\n",
    "    env.render()\n",
    "    aciton = env.action_space.sample()\n",
    "    obs, reward, done, _ = env.step(aciton)\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\"Reward got: %.2f\" % total_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The new shape of the observation\", np.shape(obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(obs[:,:,0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use the new environment with OpenAi Baselines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import the required packages.\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from baselines import logger\n",
    "# Import the ALg\n",
    "from baselines.ppo2 import ppo2 \n",
    "# Import the vectorized env\n",
    "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "# Import the policy\n",
    "# from baselines.ppo1.cnn_policy import CnnPolicy\n",
    "# Tesnorflow session\n",
    "from baselines.common.tf_util import make_session\n",
    "# Monitor wrapper reports lengths and rewards of each episode in the info dict \n",
    "from baselines.bench import Monitor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = \"./\"\n",
    "def make_env():\n",
    "    env = gym.make(\"CarRacing-v0\")\n",
    "    env.seed(0)\n",
    "    env = PreProcessObservation(env)\n",
    "    env = Monitor(env, SAVE_PATH)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the policy\n",
    "network='cnn'\n",
    "# The number of steps to run for each environment per update \n",
    "nsteps=640\n",
    "# Number of training minibatches per update\n",
    "nminibatches = 8\n",
    "# the number of timesteps to run\n",
    "total_timesteps = 1000\n",
    "# the model requires a vectorized environment ()\n",
    "env = DummyVecEnv([make_env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True # pylint: disable=E1101\n",
    "# Take more timesteps than we need to be sure that\n",
    "# we stop due to an exception.\n",
    "sess_test = make_session(make_default=True, graph=tf.Graph())\n",
    "\n",
    "model = ppo2.learn(network='cnn',\n",
    "           env=env,\n",
    "           nsteps=nsteps,\n",
    "           nminibatches=nminibatches,\n",
    "           lam=0.95,\n",
    "           gamma=0.99,\n",
    "           noptepochs=3,\n",
    "           log_interval=1,\n",
    "           ent_coef=0.01,\n",
    "           lr=lambda _: 2e-4,\n",
    "           cliprange=lambda _: 0.1,\n",
    "           total_timesteps=int(total_timesteps),\n",
    "           save_interval=10)\n",
    "\n",
    "env.close()\n",
    "#tf.InteractiveSession.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* eplenmean: mean episode length\n",
    "* ep_rewmean: mean reward per episode\n",
    "* fps: frames per second (step per second)\n",
    "* nupdates: number of gradient updates\n",
    "* serial_timesteps, i think it the same as total_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Environmet:\n",
    "One way to train your algorithm faster is to run multiple workers in parallel to get many transitions faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized Environments are a method for stacking multiple independent environments into a single environment.\n",
    "from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from baselines.common.vec_env.vec_frame_stack import VecFrameStack\n",
    "#from stable_baselines.common.vec_env import SubprocVecEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a multiprocess vectorized wrapper for multiple environments, distributing each environment to its own process, allowing significant speed up when the environment is computationally complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vec_env():\n",
    "    env = gym.make(\"CarRacing-v0\")\n",
    "    env.seed(0)\n",
    "    env = Monitor(env, logger.get_dir() and os.path.join(logger.get_dir(), str(0)), allow_early_resets=True)\n",
    "    env = PreProcessObservation(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_session(make_default=True, graph=tf.Graph())\n",
    "env = SubprocVecEnv([make_vec_env])\n",
    "\n",
    "model = ppo2.learn(network='cnn',\n",
    "           env=env,\n",
    "           nsteps=nsteps,\n",
    "           nminibatches=nminibatches,\n",
    "           lam=0.95,\n",
    "           gamma=0.99,\n",
    "           noptepochs=3,\n",
    "           log_interval=1,\n",
    "           ent_coef=0.01,\n",
    "           lr=lambda _: 2e-4,\n",
    "           cliprange=lambda _: 0.1,\n",
    "           total_timesteps=int(total_timesteps),\n",
    "           save_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml_prak",
   "language": "python",
   "name": "venv_ml_prak"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
